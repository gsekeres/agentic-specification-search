paper_id,inference_run_id,spec_run_id,spec_id,spec_tree_path,baseline_group_id,outcome_var,treatment_var,coefficient,std_error,p_value,ci_lower,ci_upper,n_obs,r_squared,coefficient_vector_json,cluster_var,run_success,run_error
149882-V1,149882-V1_infer_17,149882-V1_G1_baseline,infer/se/hc/hc1,specification_tree/modules/inference/standard_errors.md,G1,E_Sgender_index2,B_treat,0.21794516821175172,0.01542538469401587,0.0,0.18770931207619854,0.2481810243473049,13799,0.18709320621260617,"{""coefficients"": {""Intercept"": 0.7545176817497706, ""B_treat"": 0.21794516821175172, ""B_Sgender_index2"": 0.16215060000826295, ""district_gender_1"": -0.6368424323623022, ""district_gender_2"": -0.11753825899376295, ""district_gender_3"": -0.4951211354101935, ""district_gender_4"": -0.11899392909184266, ""district_gender_5"": -0.46121204069545607, ""district_gender_6"": -0.0009889637857097573, ""district_gender_7"": -0.5908151347996644, ""gender_grade_1"": 0.06360104905829694, ""gender_grade_3"": 0.08124837483393671, ""E_Sallow_work_y_flag"": -0.24867705360304096, ""E_Sboy_more_oppo_n_flag"": -0.7191758555318207, ""E_Scontrol_daughters_n_flag"": -0.2368178618805863, ""E_Select_woman_y_flag"": 0.05869732817395185, ""E_Sfertility_flag"": -0.3075854763692454, ""E_Sgirl_marriage_age_19_flag"": 0.45247256451194007, ""E_Sman_final_deci_n_flag"": -0.11861452639061668, ""E_Smarriage_age_diff_m_flag"": 0.025867401153737823, ""E_Smarriage_more_imp_n_flag"": -0.13935237016085128, ""E_Smen_better_suited_n_flag"": -0.2928867495289787, ""E_Ssimilar_right_y_flag"": 0.21481046757947936, ""E_Sstudy_marry_flag"": 0.09376687284797668, ""E_Steacher_suitable_n_flag"": -0.24349959218029088, ""E_Stown_studies_y_flag"": -0.3627750065627227, ""E_Swives_less_edu_n_flag"": -0.42918572954317136, ""E_Swoman_role_home_n_flag"": 0.16932513488580928, ""E_Swoman_viol_n_flag"": -0.41667587625216573}, ""inference"": {""spec_id"": ""infer/se/hc/hc1"", ""params"": {}}, ""software"": {""runner_language"": ""python"", ""runner_version"": ""3.12.7"", ""packages"": {""numpy"": ""2.1.3"", ""pandas"": ""2.2.3"", ""statsmodels"": ""0.14.6"", ""linearmodels"": ""6.1"", ""pyfixest"": ""0.40.1"", ""scipy"": ""1.15.1"", ""rdrobust"": ""1.3.0"", ""openpyxl"": ""3.1.5"", ""pyreadstat"": ""1.3.3""}}, ""surface_hash"": ""sha256:7e56ea846cf0fb0e7a10cb12b2d46901108440014d2b643e505b55cec808858c""}",Sschool_id,1,
149882-V1,149882-V1_infer_18,149882-V1_G1_baseline,infer/se/hc/hc3,specification_tree/modules/inference/standard_errors.md,G1,E_Sgender_index2,B_treat,0.21794516821175172,0.015460727290612293,0.0,0.1876400357704633,0.24825030065304016,13799,0.18709320621260617,"{""coefficients"": {""Intercept"": 0.7545176817497706, ""B_treat"": 0.21794516821175172, ""B_Sgender_index2"": 0.16215060000826295, ""district_gender_1"": -0.6368424323623022, ""district_gender_2"": -0.11753825899376295, ""district_gender_3"": -0.4951211354101935, ""district_gender_4"": -0.11899392909184266, ""district_gender_5"": -0.46121204069545607, ""district_gender_6"": -0.0009889637857097573, ""district_gender_7"": -0.5908151347996644, ""gender_grade_1"": 0.06360104905829694, ""gender_grade_3"": 0.08124837483393671, ""E_Sallow_work_y_flag"": -0.24867705360304096, ""E_Sboy_more_oppo_n_flag"": -0.7191758555318207, ""E_Scontrol_daughters_n_flag"": -0.2368178618805863, ""E_Select_woman_y_flag"": 0.05869732817395185, ""E_Sfertility_flag"": -0.3075854763692454, ""E_Sgirl_marriage_age_19_flag"": 0.45247256451194007, ""E_Sman_final_deci_n_flag"": -0.11861452639061668, ""E_Smarriage_age_diff_m_flag"": 0.025867401153737823, ""E_Smarriage_more_imp_n_flag"": -0.13935237016085128, ""E_Smen_better_suited_n_flag"": -0.2928867495289787, ""E_Ssimilar_right_y_flag"": 0.21481046757947936, ""E_Sstudy_marry_flag"": 0.09376687284797668, ""E_Steacher_suitable_n_flag"": -0.24349959218029088, ""E_Stown_studies_y_flag"": -0.3627750065627227, ""E_Swives_less_edu_n_flag"": -0.42918572954317136, ""E_Swoman_role_home_n_flag"": 0.16932513488580928, ""E_Swoman_viol_n_flag"": -0.41667587625216573}, ""inference"": {""spec_id"": ""infer/se/hc/hc3"", ""params"": {}}, ""software"": {""runner_language"": ""python"", ""runner_version"": ""3.12.7"", ""packages"": {""numpy"": ""2.1.3"", ""pandas"": ""2.2.3"", ""statsmodels"": ""0.14.6"", ""linearmodels"": ""6.1"", ""pyfixest"": ""0.40.1"", ""scipy"": ""1.15.1"", ""rdrobust"": ""1.3.0"", ""openpyxl"": ""3.1.5"", ""pyreadstat"": ""1.3.3""}}, ""surface_hash"": ""sha256:7e56ea846cf0fb0e7a10cb12b2d46901108440014d2b643e505b55cec808858c""}",Sschool_id,1,
149882-V1,149882-V1_infer_19,149882-V1_G1_baseline,infer/se/cluster/district,specification_tree/modules/inference/standard_errors.md,G1,E_Sgender_index2,B_treat,0.21794516821175172,0.018413798210028476,0.0012966115850732152,0.1593442441319966,0.27654609229150684,13799,0.18709320621260617,"{""coefficients"": {""Intercept"": 0.7545176817497706, ""B_treat"": 0.21794516821175172, ""B_Sgender_index2"": 0.16215060000826295, ""district_gender_1"": -0.6368424323623022, ""district_gender_2"": -0.11753825899376295, ""district_gender_3"": -0.4951211354101935, ""district_gender_4"": -0.11899392909184266, ""district_gender_5"": -0.46121204069545607, ""district_gender_6"": -0.0009889637857097573, ""district_gender_7"": -0.5908151347996644, ""gender_grade_1"": 0.06360104905829694, ""gender_grade_3"": 0.08124837483393671, ""E_Sallow_work_y_flag"": -0.24867705360304096, ""E_Sboy_more_oppo_n_flag"": -0.7191758555318207, ""E_Scontrol_daughters_n_flag"": -0.2368178618805863, ""E_Select_woman_y_flag"": 0.05869732817395185, ""E_Sfertility_flag"": -0.3075854763692454, ""E_Sgirl_marriage_age_19_flag"": 0.45247256451194007, ""E_Sman_final_deci_n_flag"": -0.11861452639061668, ""E_Smarriage_age_diff_m_flag"": 0.025867401153737823, ""E_Smarriage_more_imp_n_flag"": -0.13935237016085128, ""E_Smen_better_suited_n_flag"": -0.2928867495289787, ""E_Ssimilar_right_y_flag"": 0.21481046757947936, ""E_Sstudy_marry_flag"": 0.09376687284797668, ""E_Steacher_suitable_n_flag"": -0.24349959218029088, ""E_Stown_studies_y_flag"": -0.3627750065627227, ""E_Swives_less_edu_n_flag"": -0.42918572954317136, ""E_Swoman_role_home_n_flag"": 0.16932513488580928, ""E_Swoman_viol_n_flag"": -0.41667587625216573}, ""inference"": {""spec_id"": ""infer/se/cluster/district"", ""params"": {""cluster_var"": ""B_Sdistrict""}}, ""software"": {""runner_language"": ""python"", ""runner_version"": ""3.12.7"", ""packages"": {""numpy"": ""2.1.3"", ""pandas"": ""2.2.3"", ""statsmodels"": ""0.14.6"", ""linearmodels"": ""6.1"", ""pyfixest"": ""0.40.1"", ""scipy"": ""1.15.1"", ""rdrobust"": ""1.3.0"", ""openpyxl"": ""3.1.5"", ""pyreadstat"": ""1.3.3""}}, ""surface_hash"": ""sha256:7e56ea846cf0fb0e7a10cb12b2d46901108440014d2b643e505b55cec808858c""}",Sschool_id,1,
149882-V1,149882-V1_infer_36,149882-V1_G2_baseline,infer/se/hc/hc1,specification_tree/modules/inference/standard_errors.md,G2,E_Sbehavior_index2,B_treat,0.18444064137743707,0.01691239157746008,0.0,0.1512900484694921,0.21759123428538205,13784,0.06740061925722729,"{""coefficients"": {""Intercept"": -0.40184558447689284, ""B_treat"": 0.18444064137743707, ""B_Sbehavior_index2"": 0.076028714805201, ""district_gender_1"": 0.3336272636822163, ""district_gender_2"": -0.11223923599840417, ""district_gender_3"": 0.3514057884503106, ""district_gender_4"": -0.11350858272871603, ""district_gender_5"": 0.535341346743594, ""district_gender_6"": -0.013270221773795342, ""district_gender_7"": 0.43805630341260887, ""gender_grade_1"": -0.005964619907813472, ""gender_grade_3"": 0.035644198083582, ""E_Sabsent_sch_hhwork_comm_flag"": 0.014669142749695592, ""E_Scook_clean_comm_flag"": 0.9524566359505481, ""E_Sdiscourage_college_comm_flag"": 0.1313494728215617, ""E_Sdiscourage_work_comm_flag"": 0.13531147673297822, ""E_Ssit_opp_gender_comm_flag"": -0.5641730993284222}, ""inference"": {""spec_id"": ""infer/se/hc/hc1"", ""params"": {}}, ""software"": {""runner_language"": ""python"", ""runner_version"": ""3.12.7"", ""packages"": {""numpy"": ""2.1.3"", ""pandas"": ""2.2.3"", ""statsmodels"": ""0.14.6"", ""linearmodels"": ""6.1"", ""pyfixest"": ""0.40.1"", ""scipy"": ""1.15.1"", ""rdrobust"": ""1.3.0"", ""openpyxl"": ""3.1.5"", ""pyreadstat"": ""1.3.3""}}, ""surface_hash"": ""sha256:7e56ea846cf0fb0e7a10cb12b2d46901108440014d2b643e505b55cec808858c""}",Sschool_id,1,
149882-V1,149882-V1_infer_37,149882-V1_G2_baseline,infer/se/hc/hc3,specification_tree/modules/inference/standard_errors.md,G2,E_Sbehavior_index2,B_treat,0.18444064137743707,0.01693016991394231,0.0,0.15125520050652932,0.21762608224834482,13784,0.06740061925722729,"{""coefficients"": {""Intercept"": -0.40184558447689284, ""B_treat"": 0.18444064137743707, ""B_Sbehavior_index2"": 0.076028714805201, ""district_gender_1"": 0.3336272636822163, ""district_gender_2"": -0.11223923599840417, ""district_gender_3"": 0.3514057884503106, ""district_gender_4"": -0.11350858272871603, ""district_gender_5"": 0.535341346743594, ""district_gender_6"": -0.013270221773795342, ""district_gender_7"": 0.43805630341260887, ""gender_grade_1"": -0.005964619907813472, ""gender_grade_3"": 0.035644198083582, ""E_Sabsent_sch_hhwork_comm_flag"": 0.014669142749695592, ""E_Scook_clean_comm_flag"": 0.9524566359505481, ""E_Sdiscourage_college_comm_flag"": 0.1313494728215617, ""E_Sdiscourage_work_comm_flag"": 0.13531147673297822, ""E_Ssit_opp_gender_comm_flag"": -0.5641730993284222}, ""inference"": {""spec_id"": ""infer/se/hc/hc3"", ""params"": {}}, ""software"": {""runner_language"": ""python"", ""runner_version"": ""3.12.7"", ""packages"": {""numpy"": ""2.1.3"", ""pandas"": ""2.2.3"", ""statsmodels"": ""0.14.6"", ""linearmodels"": ""6.1"", ""pyfixest"": ""0.40.1"", ""scipy"": ""1.15.1"", ""rdrobust"": ""1.3.0"", ""openpyxl"": ""3.1.5"", ""pyreadstat"": ""1.3.3""}}, ""surface_hash"": ""sha256:7e56ea846cf0fb0e7a10cb12b2d46901108440014d2b643e505b55cec808858c""}",Sschool_id,1,
149882-V1,149882-V1_infer_38,149882-V1_G2_baseline,infer/se/cluster/district,specification_tree/modules/inference/standard_errors.md,G2,E_Sbehavior_index2,B_treat,0.18444064137743707,0.03157372091341592,0.009996745538373153,0.08395896991246007,0.2849223128424141,13784,0.06740061925722729,"{""coefficients"": {""Intercept"": -0.40184558447689284, ""B_treat"": 0.18444064137743707, ""B_Sbehavior_index2"": 0.076028714805201, ""district_gender_1"": 0.3336272636822163, ""district_gender_2"": -0.11223923599840417, ""district_gender_3"": 0.3514057884503106, ""district_gender_4"": -0.11350858272871603, ""district_gender_5"": 0.535341346743594, ""district_gender_6"": -0.013270221773795342, ""district_gender_7"": 0.43805630341260887, ""gender_grade_1"": -0.005964619907813472, ""gender_grade_3"": 0.035644198083582, ""E_Sabsent_sch_hhwork_comm_flag"": 0.014669142749695592, ""E_Scook_clean_comm_flag"": 0.9524566359505481, ""E_Sdiscourage_college_comm_flag"": 0.1313494728215617, ""E_Sdiscourage_work_comm_flag"": 0.13531147673297822, ""E_Ssit_opp_gender_comm_flag"": -0.5641730993284222}, ""inference"": {""spec_id"": ""infer/se/cluster/district"", ""params"": {""cluster_var"": ""B_Sdistrict""}}, ""software"": {""runner_language"": ""python"", ""runner_version"": ""3.12.7"", ""packages"": {""numpy"": ""2.1.3"", ""pandas"": ""2.2.3"", ""statsmodels"": ""0.14.6"", ""linearmodels"": ""6.1"", ""pyfixest"": ""0.40.1"", ""scipy"": ""1.15.1"", ""rdrobust"": ""1.3.0"", ""openpyxl"": ""3.1.5"", ""pyreadstat"": ""1.3.3""}}, ""surface_hash"": ""sha256:7e56ea846cf0fb0e7a10cb12b2d46901108440014d2b643e505b55cec808858c""}",Sschool_id,1,
